# ðŸ§  ROS Navigation & Perception Pipeline

This document details the internal ROS graph architecture constructed for the LunaBot autonomy stack.

## 1. Node Graph Analysis
The system architecture moves beyond basic tutorials by implementing a modular, distributed node structure.

![ROS Node Graph](images/node_graph.png)

### Key Nodes & Responsibilities:
* **`/mission_manager`:**  
  Central state machine that prioritizes objectives based on `/battery_level` and `/semantic_detections`.

* **`/semantic_annotator`:**  
  Fusion node that projects 2D YOLO bounding boxes into 3D semantic markers using depth and camera intrinsics.

* **`/goal_based_avoider`:**  
  Custom local planner that performs reactive obstacle avoidance without `move_base`, suited for lunar terrain.

## 2. Kinematics & Transforms (TF Tree)
A precise TF tree is required for accurate SLAM. The system follows the standard ROS hierarchy:

`map â†’ odom â†’ base_link â†’ {camera frames, lidar_link}`

![TF Tree](images/tf_tree.png)

* **Odom Source:**  
  Odometry is generated by Unityâ€™s physics engine and published to ROS through the WebSocket bridge.

* **Sensor Offsets:**  
  Sensor transforms (camera, depth camera, LiDAR) are published as **static transforms**.  
  RViz may display these at very high virtual update rates (e.g., ~10,000 Hz),  
  but they are published once as static TFs.

## 3. RGB-D SLAM Configuration
We utilize **RTAB-Map** configured for RGB-D mode:

* **Inputs:**  
  Synchronized RGB frames, Depth images (32FC1), and 2D LaserScan.

* **Loop Closure:**  
  Visual bag-of-words + geometric scan matching.

* **Outputs:**  
  2D Occupancy Grid (`/grid_map`) and 3D Global Point Cloud (`/cloud_map`).
